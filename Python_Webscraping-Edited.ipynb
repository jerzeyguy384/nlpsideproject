{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp \"Scraping\"\n",
    "\n",
    "This is a script for using Yelp data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "# import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we need to read in our restaurant URL list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv('C:/Users/Pedro/Documents/nlpsideproject/Yelp_Wine_Bar_addresses18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/district-winery-washi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         url_address\n",
       "0  https://www.yelp.com/biz/district-winery-washi..."
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scope = ['https://spreadsheets.google.com/feeds']\n",
    " \n",
    "# credentials = ServiceAccountCredentials.from_json_keyfile_name('My_First_Project-041c0e0956f7.json', scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gc = gspread.authorize(credentials)\n",
    "\n",
    "# rest_url = gc.open(\"Yelp_Wine_Bar_addresses\").sheet1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need to produce the list of ALL URLs we're going to scrpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a function that identifies the number of pages for a given restuarant\n",
    "def get_num_pages(rest_url):\n",
    "    r = requests.get(rest_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    pages = soup.find('div', {'class':'page-of-pages arrange_unit arrange_unit--fill'}).text\n",
    "    pages = int(pages.split(\"of\",1)[1].strip())\n",
    "    return(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a function that takes a given number and base URL, it returns all necessary URLS\n",
    "# return list of all restaurant URLs\n",
    "def get_webpages(rest_url):\n",
    "    pages = get_num_pages(rest_url)\n",
    "    urls = []\n",
    "    urls.append(rest_url)\n",
    "    for num in range(1,pages):\n",
    "        urls.append(rest_url + \"?start=\"+str(num*20))\n",
    "    return(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_webpages = get_webpages(wine_data.url_address[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/biz/district-winery-washington',\n",
       " 'https://www.yelp.com/biz/district-winery-washington?start=20']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on time 1\n",
      "We are on time 2\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,3):\n",
    "    print('We are on time %d' % (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/district-winery-washi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         url_address\n",
       "0  https://www.yelp.com/biz/district-winery-washi..."
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.yelp.com/biz/district-winery-washi...\n",
       "Name: url_address, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data.url_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "webpages = []\n",
    "for x in wine_data.url_address:\n",
    "    webpages.append(get_webpages(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.yelp.com/biz/jug-and-table-washington',\n",
       "  'https://www.yelp.com/biz/jug-and-table-washington?start=20'],\n",
       " ['https://www.yelp.com/biz/district-winery-washington',\n",
       "  'https://www.yelp.com/biz/district-winery-washington?start=20'],\n",
       " ['https://www.yelp.com/biz/ana-at-district-winery-washington-2']]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_webpages = list(itertools.chain.from_iterable(webpages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_webpages_df = pd.DataFrame(master_webpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_webpages_df.to_csv('master_webpages18.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use that master URL list to scrape each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_webpages_df = pd.read_csv('C:/Users/Pedro/Documents/nlpsideproject/master_webpages18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/biz/district-winery-washington',\n",
       " 'https://www.yelp.com/biz/district-winery-washington?start=20']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build df\n",
    "yelp_data = pd.DataFrame(columns=['rest_name','username','review','stars','date','location'])\n",
    "yelp_data.to_csv('yelp_data_second_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a list of URLs, return a dataframe\n",
    "def scrape_yelp(rest_url_list):\n",
    "    # count the url we're at\n",
    "    count = 0\n",
    "    \n",
    "    for url in rest_url_list:\n",
    "\n",
    "        \n",
    "        # make temp df\n",
    "        temp_df = pd.DataFrame(columns=['rest_name','username','review','stars','date','location'])\n",
    "    \n",
    "        # declare empty lists to fill\n",
    "        rest_names = []\n",
    "        user_names = []\n",
    "        reviews = []\n",
    "        stars_list = []\n",
    "        dates = []\n",
    "        locations = []\n",
    "        \n",
    "        # sleep, get webpage\n",
    "        rand = np.random.randint(15,60)\n",
    "        sleep(rand)\n",
    "        f = requests.get(url)\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    " \n",
    "        # get name\n",
    "        for user in soup.find_all('a', {'class':'user-display-name js-analytics-click'}):\n",
    "            user_names.append(user['href'])\n",
    "        user_names = user_names[1:] #it's either 1:, 2:, or 3: to make sure that the data works. Maybe an elif would work?\n",
    "        \n",
    "        # get review\n",
    "        for review in soup.find_all('p', attrs={'itemprop':'description'}):\n",
    "            reviews.append(review.text)\n",
    "            \n",
    "        # get stars\n",
    "        for stars in soup.find_all('div', attrs={'itemprop':'reviewRating'}):\n",
    "            stars_list.append(stars.find('meta')['content'])\n",
    "            \n",
    "        # get date\n",
    "        for date in soup.find_all('meta', attrs={'itemprop':'datePublished'}):\n",
    "            dates.append((date)['content'])\n",
    "            \n",
    "        # get location    \n",
    "        for loc in soup.find_all('li', {'class':'user-location responsive-hidden-small'}):\n",
    "            locations.append(loc.text.strip())\n",
    "        \n",
    "        # get rest_name\n",
    "        try:\n",
    "            rest_name = soup.find('h1',{'class':'biz-page-title embossed-text-white shortenough'}).text.strip()\n",
    "        except:\n",
    "            rest_name = soup.find('h1',{'class':'biz-page-title embossed-text-white'}).text.strip()\n",
    "        rest_names = [rest_name]*len(reviews)\n",
    "        \n",
    "        # add to temp_df        \n",
    "        temp_df.rest_name = rest_names\n",
    "        temp_df.username = user_names\n",
    "        temp_df.review = reviews\n",
    "        temp_df.stars = stars_list\n",
    "        temp_df.date = dates\n",
    "        temp_df.location = locations\n",
    "        \n",
    "        # read in previous df\n",
    "        # prev_df = pd.read_csv('data/yelp_data.csv')\n",
    " \n",
    "        # add to df\n",
    "        # final_df = prev_df.append(temp_df)\n",
    "        \n",
    "        # export csv\n",
    "        # final_df.to_csv('data/yelp_data.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        # export scraped data\n",
    "        temp_df.to_csv('data/yelp_data_second_%s.csv' %str(count), index=False, encoding='utf-8')\n",
    "        #temp_df.to_csv('[rest_name]/yelp_data_second_%s.csv' %str(count), index=False, encoding='utf-8')\n",
    "        \n",
    "        # increment count\n",
    "        count = count+1\n",
    "        print('Success! Scraped and exported %d webpage(s)' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Scraped and exported 1 webpage(s)\n",
      "Success! Scraped and exported 2 webpage(s)\n"
     ]
    }
   ],
   "source": [
    "scrape_yelp(master_webpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resolving different classname error\n",
    "try:\n",
    "    rest_name = soup.find('h1',{'class':'biz-page-title embossed-text-white shortenough'}).text.strip()\n",
    "except:\n",
    "    rest_name = soup.find('h1',{'class':'biz-page-title embossed-text-white'}).text.strip()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
